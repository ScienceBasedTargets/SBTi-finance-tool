{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBTi-Finance Tool - Portfolio Aggregation\n",
    "In this notebook we'll give some examples on how the portfolio aggregation methods can be used.\n",
    "\n",
    "Please see the [methodology](https://sciencebasedtargets.org/wp-content/uploads/2020/09/Temperature-Rating-Methodology-V1.pdf), [guidance](https://sciencebasedtargets.org/wp-content/uploads/2020/10/Financial-Sector-Science-Based-Targets-Guidance-Pilot-Version.pdf) and the [technical documentation](https://sciencebasedtargets.github.io/SBTi-finance-tool/)  for more details on the different aggregation methods.\n",
    "\n",
    "See 1_analysis_example (on [Colab](https://colab.research.google.com/github/ScienceBasedTargets/SBTi-finance-tool/blob/main/examples/1_analysis_example.ipynb) or [Github](https://github.com/ScienceBasedTargets/SBTi-finance-tool/blob/main/examples/1_analysis_example.ipynb)) for more in depth example of how to work with Jupyter Notebooks in general and SBTi notebooks in particular. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "First we will set up the imports, data providers, and load the portfolio. \n",
    "\n",
    "For more examples of this process, please refer to notebook 1 & 2 (analysis and quick calculation example).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sbti-finance-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import SBTi\n",
    "from SBTi.data.excel import ExcelProvider\n",
    "from SBTi.portfolio_aggregation import PortfolioAggregationMethod\n",
    "from SBTi.portfolio_coverage_tvp import PortfolioCoverageTVP\n",
    "from SBTi.temperature_score import TemperatureScore, Scenario, ScenarioType, EngagementType\n",
    "from SBTi.target_validation import TargetProtocol\n",
    "from SBTi.interfaces import ETimeFrames, EScope\n",
    "%aimport -pandas\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dummy data\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "if not os.path.isfile(\"data/data_provider_example.xlsx\"):\n",
    "    urllib.request.urlretrieve(\"https://github.com/ScienceBasedTargets/SBTi-finance-tool/raw/main/examples/data/data_provider_example.xlsx\", \"data/data_provider_example.xlsx\")\n",
    "if not os.path.isfile(\"data/example_portfolio.csv\"):\n",
    "    urllib.request.urlretrieve(\"https://github.com/ScienceBasedTargets/SBTi-finance-tool/raw/main/examples/data/example_portfolio.csv\", \"data/example_portfolio.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = ExcelProvider(path=\"data/data_provider_example.xlsx\")\n",
    "df_portfolio = pd.read_csv(\"data/example_portfolio.csv\", encoding=\"iso-8859-1\")\n",
    "\n",
    "# Print original columns to diagnose\n",
    "print(\"Original columns:\", df_portfolio.columns.tolist())\n",
    "\n",
    "# Create a more cautious renaming dictionary based on what columns exist\n",
    "rename_dict = {}\n",
    "if \"Company Name\" in df_portfolio.columns:\n",
    "    rename_dict[\"Company Name\"] = \"company_name\"\n",
    "if \"ISIN\" in df_portfolio.columns:\n",
    "    rename_dict[\"ISIN\"] = \"isin\" \n",
    "if \"LEI\" in df_portfolio.columns:\n",
    "    rename_dict[\"LEI\"] = \"lei\"\n",
    "if \"Sector\" in df_portfolio.columns:\n",
    "    rename_dict[\"Sector\"] = \"sector\"\n",
    "if \"Target\" in df_portfolio.columns:\n",
    "    rename_dict[\"Target\"] = \"full_target_language\"\n",
    "if \"Net-Zero Committed\" in df_portfolio.columns:\n",
    "    rename_dict[\"Net-Zero Committed\"] = \"net_zero_status\"\n",
    "if \"Near term - Target Status\" in df_portfolio.columns:\n",
    "    rename_dict[\"Near term - Target Status\"] = \"near_term_status\"\n",
    "if \"Target Classification\" in df_portfolio.columns:\n",
    "    rename_dict[\"Target Classification\"] = \"target_classification_long\"\n",
    "if \"Extension\" in df_portfolio.columns:\n",
    "    rename_dict[\"Extension\"] = \"reason_for_extension_or_removal\"\n",
    "if \"Date\" in df_portfolio.columns:\n",
    "    rename_dict[\"Date\"] = \"date_updated\"\n",
    "\n",
    "# Apply column renaming\n",
    "df_portfolio = df_portfolio.rename(columns=rename_dict)\n",
    "\n",
    "# Handle legacy column names if they exist\n",
    "if 'company_isin' in df_portfolio.columns:\n",
    "    df_portfolio.rename(columns={'company_isin': 'isin'}, inplace=True)\n",
    "if 'company_lei' in df_portfolio.columns:\n",
    "    df_portfolio.rename(columns={'company_lei': 'lei'}, inplace=True)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = ['company_id', 'company_name', 'isin', 'lei', 'investment_value']\n",
    "for col in required_columns:\n",
    "    if col not in df_portfolio.columns:\n",
    "        df_portfolio[col] = None\n",
    "\n",
    "# Convert identifiers to string\n",
    "if 'isin' in df_portfolio.columns:\n",
    "    df_portfolio['isin'] = df_portfolio['isin'].astype(str)\n",
    "if 'lei' in df_portfolio.columns:\n",
    "    df_portfolio['lei'] = df_portfolio['lei'].astype(str)\n",
    "\n",
    "# Check for duplicate values in the 'company_id' column\n",
    "duplicate_ids = df_portfolio[df_portfolio.duplicated('company_id', keep=False)]\n",
    "if not duplicate_ids.empty:\n",
    "    print(\"Error: Duplicate values found in the 'company_id' column:\")\n",
    "    print(duplicate_ids)\n",
    "else:\n",
    "    print(\"No duplicate values found in the 'company_id' column.\")\n",
    "\n",
    "# Display final columns to verify\n",
    "print(\"Final columns:\", df_portfolio.columns.tolist())\n",
    "\n",
    "# Convert to portfolio objects and get data\n",
    "companies = SBTi.utils.dataframe_to_portfolio(df_portfolio)\n",
    "try:\n",
    "    portfolio_data = SBTi.utils.get_data([provider], companies)\n",
    "    scenarios = {}\n",
    "    print(\"Successfully loaded portfolio data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading portfolio data: {str(e)}\")\n",
    "    # Add more error handling if needed\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the CTA and set up a SBTi target frame of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides an absolute frame of reference for SBTi targets so that they are considered as cardinal compared to others in the calculation of temperature scores.\n",
    "def inject_sbti_validation_for_timeframe_scope_data(amended_portfolio, original_portfolio, debug=True):\n",
    "    \"\"\"\n",
    "    Specially designed for the SBTi tool where amended_portfolio contains multiple rows \n",
    "    per company (one for each time frame and scope combination).\n",
    "    \"\"\"\n",
    "    if 'sbti_validated' not in original_portfolio.columns:\n",
    "        print(\"âš  No 'sbti_validated' column found in original portfolio\")\n",
    "        return amended_portfolio\n",
    "    \n",
    "    # Store original values before modification\n",
    "    original_validated_count = original_portfolio['sbti_validated'].sum()\n",
    "    original_companies_count = len(original_portfolio)\n",
    "    \n",
    "    # Get count of unique companies in amended portfolio\n",
    "    unique_companies_amended = amended_portfolio['company_id'].nunique()\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Original portfolio: {original_companies_count} companies, {original_validated_count} validated\")\n",
    "        print(f\"Amended portfolio: {len(amended_portfolio)} rows, {unique_companies_amended} unique companies\")\n",
    "        \n",
    "        # Check for duplicated rows by company_id\n",
    "        if len(amended_portfolio) > unique_companies_amended:\n",
    "            print(f\"Multiple rows per company detected in amended portfolio\")\n",
    "            print(f\"Rows per company: {len(amended_portfolio) / unique_companies_amended:.2f}\")\n",
    "            \n",
    "            # Show distribution of time_frame and scope if they exist\n",
    "            if 'time_frame' in amended_portfolio.columns:\n",
    "                print(\"\\nTime frame distribution:\")\n",
    "                print(amended_portfolio['time_frame'].value_counts())\n",
    "            if 'scope' in amended_portfolio.columns:\n",
    "                print(\"\\nScope distribution:\")\n",
    "                print(amended_portfolio['scope'].value_counts())\n",
    "    \n",
    "    # Create a validation mapping\n",
    "    validation_map = dict(zip(original_portfolio['company_id'], original_portfolio['sbti_validated']))\n",
    "    \n",
    "    # Apply validation to all rows in amended portfolio\n",
    "    amended_portfolio['sbti_validated'] = amended_portfolio['company_id'].map(validation_map).fillna(False)\n",
    "    \n",
    "    # Count unique validated companies after modification\n",
    "    validated_companies = amended_portfolio[amended_portfolio['sbti_validated']]['company_id'].nunique()\n",
    "    \n",
    "    # Print validation summary\n",
    "    print(f\"\\nOriginal validated companies: {original_validated_count}\")\n",
    "    print(f\"Unique companies validated in amended portfolio: {validated_companies}\")\n",
    "    \n",
    "    if original_validated_count == validated_companies:\n",
    "        print(\"âœ“ CTA validation successfully preserved at company level\")\n",
    "    else:\n",
    "        print(\"âš  CTA validation mismatch at company level\")\n",
    "        \n",
    "        # Additional debugging\n",
    "        if debug:\n",
    "            print(\"\\nChecking for specific discrepancies...\")\n",
    "            # Get list of company IDs that should be validated\n",
    "            original_validated_ids = set(original_portfolio[original_portfolio['sbti_validated']]['company_id'])\n",
    "            amended_validated_ids = set(amended_portfolio[amended_portfolio['sbti_validated']]['company_id'])\n",
    "            \n",
    "            missing_validations = original_validated_ids - amended_validated_ids\n",
    "            extra_validations = amended_validated_ids - original_validated_ids\n",
    "            \n",
    "            if missing_validations:\n",
    "                print(f\"Companies that should be validated but aren't: {len(missing_validations)}\")\n",
    "                print(missing_validations)\n",
    "            \n",
    "            if extra_validations:\n",
    "                print(f\"Companies that shouldn't be validated but are: {len(extra_validations)}\")\n",
    "                print(extra_validations)\n",
    "    \n",
    "    return amended_portfolio\n",
    "# STANDALONE SBTi VALIDATION - Download and process CTA data\n",
    "print(\"Downloading SBTi Companies Taking Action (CTA) data...\")\n",
    "CTA_FILE_URL = \"https://cdn.sciencebasedtargets.org/download/target-dashboard\"\n",
    "\n",
    "try:\n",
    "    resp = requests.get(CTA_FILE_URL)\n",
    "    if resp.status_code == 200:\n",
    "        cta_file = pd.read_excel(resp.content)\n",
    "        print(f\"Downloaded CTA data with {len(cta_file)} rows\")\n",
    "        \n",
    "        # Extract relevant columns\n",
    "        targets = cta_file[['company_name', 'isin', 'lei', 'action', 'target', 'date_published']]\n",
    "        \n",
    "        # Filter for companies with targets\n",
    "        companies_with_targets = targets[targets['action'] == 'Target']\n",
    "        \n",
    "        # Get unique identifiers\n",
    "        all_isin_set = set(companies_with_targets['isin'].dropna())\n",
    "        all_lei_set = set(companies_with_targets['lei'].dropna())\n",
    "        \n",
    "        # Create a set of lowercase company names\n",
    "        companies_with_targets['company_name_lower'] = companies_with_targets['company_name'].str.lower()\n",
    "        company_name_set = set(companies_with_targets['company_name_lower'].dropna())\n",
    "        \n",
    "        # Add df_portfolio columns if they don't exist\n",
    "        if 'isin' not in df_portfolio.columns and 'company_isin' in df_portfolio.columns:\n",
    "            df_portfolio['isin'] = df_portfolio['company_isin']\n",
    "        if 'lei' not in df_portfolio.columns and 'company_lei' in df_portfolio.columns:\n",
    "            df_portfolio['lei'] = df_portfolio['company_lei']\n",
    "        \n",
    "        # Function to check if ISIN, LEI, or company name is validated\n",
    "        def is_validated(row):\n",
    "            # First check LEI\n",
    "            if pd.notna(row.get('lei')) and str(row.get('lei')).lower() in [str(x).lower() for x in all_lei_set]:\n",
    "                return True\n",
    "            \n",
    "            # Then check ISIN\n",
    "            if pd.notna(row.get('isin')) and str(row.get('isin')).lower() in [str(x).lower() for x in all_isin_set]:\n",
    "                return True\n",
    "            \n",
    "            # Finally check company name\n",
    "            if pd.notna(row.get('company_name')):\n",
    "                company_name_lower = str(row.get('company_name')).lower()\n",
    "                if company_name_lower in company_name_set:\n",
    "                    return True\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        # Add the validated column to the df_portfolio\n",
    "        df_portfolio['sbti_validated'] = df_portfolio.apply(is_validated, axis=1)\n",
    "        \n",
    "        # Convert df_portfolio to company objects again (after adding sbti_validated)\n",
    "        companies = SBTi.utils.dataframe_to_portfolio(df_portfolio)\n",
    "        \n",
    "        # Print validation summary\n",
    "        validated_count = df_portfolio['sbti_validated'].sum()\n",
    "        print(f\"Companies with SBTi-validated targets: {validated_count} out of {len(df_portfolio)} ({validated_count/len(df_portfolio)*100:.2f}%)\")\n",
    "        \n",
    "        if 'investment_value' in df_portfolio.columns:\n",
    "            total_investment = df_portfolio['investment_value'].sum()\n",
    "            validated_investment = df_portfolio[df_portfolio['sbti_validated']]['investment_value'].sum()\n",
    "            print(f\"Portfolio coverage by investment value: {validated_investment/total_investment*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Failed to download CTA file: HTTP {resp.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing CTA file: {str(e)}\")\n",
    "\n",
    "# Update provider data with our validation results\n",
    "print(\"Updating provider data with validated companies...\")\n",
    "try:\n",
    "    # Get the fundamental_data from provider\n",
    "    if hasattr(provider, 'data') and 'fundamental_data' in provider.data:\n",
    "        # Create a mapping of company_id to validation status\n",
    "        validation_map = df_portfolio[['company_id', 'sbti_validated']].set_index('company_id')['sbti_validated'].to_dict()\n",
    "        \n",
    "        # Update sbti_validated in provider data\n",
    "        updated_count = 0\n",
    "        for idx, row in provider.data['fundamental_data'].iterrows():\n",
    "            company_id = row['company_id']\n",
    "            if company_id in validation_map:\n",
    "                # Ensure we're setting a proper boolean value\n",
    "                is_validated = bool(validation_map[company_id])\n",
    "                provider.data['fundamental_data'].at[idx, 'sbti_validated'] = is_validated\n",
    "                updated_count += 1\n",
    "        \n",
    "        # Force the sbti_validated column to be boolean type\n",
    "        provider.data['fundamental_data']['sbti_validated'] = provider.data['fundamental_data']['sbti_validated'].astype(bool)\n",
    "        \n",
    "        print(f\"Updated sbti_validated for {updated_count} companies in provider data\")\n",
    "    else:\n",
    "        print(\"Provider does not have expected data structure - sbti_validated flags may be overwritten\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating provider data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_score = TemperatureScore(time_frames=list(SBTi.interfaces.ETimeFrames), scopes=[EScope.S1S2, EScope.S3, EScope.S1S2S3])\n",
    "amended_portfolio = temperature_score.calculate(data_providers=[provider], portfolio=companies)\n",
    "# Preserve the CTA validation from our direct download\n",
    "amended_portfolio = inject_sbti_validation_for_timeframe_scope_data(amended_portfolio, df_portfolio, debug=True)\n",
    "\n",
    "scores_collection = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the aggregated temperature score\n",
    "Calculate an aggregated temperature score. This can be done using different aggregation methods. The termperature scores are calculated per time-frame/scope combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### WATS\n",
    "Weighted Average Temperature Score (WATS): Temperature scores are allocated based on portfolio weights.\n",
    "This method uses the \"investment_value\" field to be defined in your portfolio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.WATS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_wats = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'WATS': df_wats})\n",
    "df_wats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TETS\n",
    "Total emissions weighted temperature score (TETS): Temperature scores are allocated based on historical emission weights using total company emissions. \n",
    "In addition to the portfolios \"investment value\" the TETS method requires company emissions, please refer to [Data Legends - Fundamental Data](https://ofbdabv.github.io/SBTi/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.TETS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_tets = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'TETS': df_tets})\n",
    "df_tets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MOTS\n",
    "Market Owned emissions weighted temperature score (MOTS): Temperature scores are allocated based on an equity ownership approach.\n",
    "In addition to the portfolios \"investment value\" the MOTS method requires company emissions and market cap, please refer to  [Data Legends - Fundamental Data](https://ofbdabv.github.io/SBTi/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.MOTS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_mots = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'MOTS': df_mots})\n",
    "df_mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EOTS\n",
    "Enterprise Owned emissions weighted temperature score (EOTS): Temperature scores are allocated based\n",
    "on an enterprise ownership approach. \n",
    "In addition to the portfolios \"investment value\" the EOTS method requires company emissions and enterprise value, please refer to  [Data Legends - Fundamental Data](https://ofbdabv.github.io/SBTi/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.EOTS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_eots = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'EOTS': df_eots})\n",
    "df_eots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ECOTS\n",
    "Enterprise Value + Cash emissions weighted temperature score (ECOTS): Temperature scores are allocated based on an enterprise value (EV) plus cash & equivalents ownership approach. \n",
    "In addition to the portfolios \"investment value\" the ECOTS method requires company emissions, company cash equivalents and enterprise value; please refer to  [Data Legends - Fundamental Data](https://sciencebasedtargets.github.io/SBTi-finance-tool/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.ECOTS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_ecots = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'ECOTS': df_ecots})\n",
    "df_ecots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### AOTS\n",
    "Total Assets emissions weighted temperature score (AOTS): Temperature scores are allocated based on a total assets ownership approach. \n",
    "In addition to the portfolios \"investment value\" the AOTS method requires company emissions and company total assets; please refer to  [Data Legends - Fundamental Data](https://sciencebasedtargets.github.io/SBTi-finance-tool/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.AOTS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_aots = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'AOTS': df_aots})\n",
    "df_aots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ROTS\n",
    "Revenue owned emissions weighted temperature score (ROTS): Temperature scores are allocated based on the share of revenue.\n",
    "In addition to the portfolios \"investment value\" the ROTS method requires company emissions and company revenue; please refer to  [Data Legends - Fundamental Data](https://sciencebasedtargets.github.io/SBTi-finance-tool/Legends.html#fundamental-data) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temperature_score.aggregation_method = PortfolioAggregationMethod.ROTS\n",
    "aggregated_scores = temperature_score.aggregate_scores(amended_portfolio)\n",
    "df_rots = pd.DataFrame(aggregated_scores.dict()).applymap(lambda x: round(x['all']['score'], 2))\n",
    "scores_collection.update({'ROTS': df_rots})\n",
    "df_rots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "See below how each aggregation method impact the scores on for each time frame and scope combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat(scores_collection, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
